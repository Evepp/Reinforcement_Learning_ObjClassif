# -*- coding: utf-8 -*-
"""Code_003_change_loop_loadmodel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/134Jq9Rv0BpQvrp0vvo3aHmRSwUEZh16K
"""

import pandas as pd
import numpy as np
#import glob
import os
#import json
#from PIL import Image
import tensorflow as tf
import tensorflow_datasets as tfds
#import pathlib
#import requests
import keras
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from tensorflow.keras import regularizers
import random
from sklearn.metrics import accuracy_score
import time
#from scipy.ndimage.filters import gaussian_filter
###DEBLURfrom skimage import restoration
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Input, Flatten
from tensorflow.keras.optimizers import Adam
from collections import deque
import cv2
import pickle


seed = 42
TryNumber = 'CX2'


import pickle
'''os.chdir("/content/drive/MyDrive/Thesis Folder/Dataset/Subset105/GPU")'''

# Load training datasets
with open('X_train_A12{}.pkl'.format(TryNumber), 'rb') as f:
    X_train = pickle.load(f)

with open('y_train_A12{}.pkl'.format(TryNumber), 'rb') as f:
    y_train = pickle.load(f)

# Load validation datasets
with open('X_val_A12{}.pkl'.format(TryNumber), 'rb') as f:
    X_val = pickle.load(f)

with open('y_val_A12{}.pkl'.format(TryNumber), 'rb') as f:
    y_val = pickle.load(f)

print(len(X_train))
print(len(X_val))

print("A.1:", X_train.shape)
print("A.2:", X_val.shape)

#making them float
X_train=X_train.astype('float32')
X_val=X_val.astype('float32')

#Normalizing the data between 0 and 1
X_train=X_train/255.0
X_val=X_val/255.0

## label names
labels=pd.read_csv("/home/u784210/Thesis/labels.txt", sep=' ', names=["Number", "Object"])
###labels=pd.read_csv("/content/drive/MyDrive/Thesis Folder/Dataset/Subset105/labels.txt", sep=' ', names=["Number", "Object"])
labels=labels.iloc[:40,:]
label = labels["Object"].values.tolist()

#Categorical encoding
label_map = {}
for i in range(len(label)):
  label_map[str(label[i])]=i

# Encode the categorical labels as numerical values using the label map
y_train_encoded = np.vectorize(label_map.get)(y_train)
y_val_encoded = np.vectorize(label_map.get)(y_val)

# Convert the numerical labels to one-hot encoded format
num_classes = len(label)
y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)
y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)

'''
seed = 42


from keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import RandomizedSearchCV

# Define a function to create model, required for KerasClassifier
def create_model(filters=64, kernel_size=(5, 5), activation='relu'):
    model = keras.Sequential([
        keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation, input_shape=(64, 64, 3)),
        keras.layers.Conv2D(filters=filters//2, kernel_size=kernel_size, activation=activation, kernel_regularizer=regularizers.l2(0.001)),
        keras.layers.MaxPooling2D(pool_size=(2, 2)),
        keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, activation=activation),
        keras.layers.Conv2D(filters=filters//2, kernel_size=kernel_size, activation=activation),
        keras.layers.MaxPooling2D(pool_size=(2, 2)),

        keras.layers.Flatten(),
        keras.layers.Dense(filters//2, activation=activation),
        keras.layers.Dense(filters//2, activation=activation),
        keras.layers.Dense(40, activation='softmax')
    ])
    optimizer = Adam(learning_rate=0.001)

    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])
    return model

# Create model

baseline_model = create_model()

# Train the model for 12 epochs with a batch size of 32
history = baseline_model.fit(
    X_train,
    y_train_onehot,
    batch_size=32,
    epochs=12,
    validation_data=(X_val, y_val_onehot)
)'''

from tensorflow.keras.models import load_model

# Load the saved model
baseline_model = load_model('baselinemodel_{}.h5'.format(TryNumber))

from sklearn.metrics import classification_report


# Make predictions on the test set CNN
y_pred_val = baseline_model.predict(X_val)
y_pred_classes_val = np.argmax(y_pred_val, axis=1)
y_pred_classes_val=keras.utils.to_categorical(y_pred_classes_val, num_classes=num_classes)

print(classification_report(y_val_onehot, y_pred_classes_val))


from sklearn.metrics import accuracy_score

accuracy = accuracy_score(y_val_onehot, y_pred_classes_val)

print("Accuracy CNN in Validation Set", accuracy )



''''
#Plot training ,validation accuracy values
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], color='blue')
plt.plot(history.history['val_accuracy'], color='orange')
plt.title('Model CNN Accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epochs')
plt.legend(['Train', 'Validation'], loc='upper left')


#Plot training, validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], color='blue')
plt.plot(history.history['val_loss'], color='orange')
plt.title('Model CNN loss')
plt.ylabel('Loss')
plt.xlabel('Epochs')
plt.legend(['Train', 'Validation'], loc='upper left')

plt.tight_layout()

filename='training_validation_{}.png'.format(TryNumber)

#save_path = os.path.join('/content/drive/MyDrive/Thesis Folder/Dataset/Subset105/', filename)

plt.savefig(filename)  # You can choose another file name or specify a directory, e.g., 'images/training_validation.png'


plt.show()'''

'''baseline_all_val_acc = np.mean(history.history["val_accuracy"])
baseline_all_val_loss = np.mean(history.history["val_loss"])
print("BASELINE RESULTS:")
print("-"*len("BASELINE RESULTS:"))
print()
print("The average validation accuracy among all epochs is: {}".format(baseline_all_val_acc))
print("The average validation loss among all epochs is: {}".format(baseline_all_val_loss))'''

"""### Blur test data

"""


"""### Data Test Split"""

import pickle

# Load testing datasets
with open('X_testA3C.pkl'.format(TryNumber), 'rb') as f:
    X_test = pickle.load(f)

with open('y_testA3C.pkl'.format(TryNumber), 'rb') as f:
    y_test = pickle.load(f)

# Load training datasets
with open('X_trainA3.pkl'.format(TryNumber), 'rb') as f:
    X_train = pickle.load(f)

with open('y_trainA3.pkl', 'rb') as f:
    y_train = pickle.load(f)

# Load validation datasets
with open('X_valA3.pkl', 'rb') as f:
    X_val = pickle.load(f)

with open('y_valA3.pkl', 'rb') as f:
    y_val = pickle.load(f)

print("A.3.a:",len(X_train))
print("A.3.b:",len(X_val))
print("A.3.c:",len(X_test))
'''A.3.a: 1036
A.3.b: 346
A.3.c: 346'''

"""## Encoding for DQL

"""

X_train=X_train.astype('float32')
X_val=X_val.astype('float32')

X_test=X_test.astype('float32')

X_train=X_train/255.0
X_val=X_val/255.0

X_test=X_test/255.0

#Categorical encoding
label_map = {}
for i in range(len(label)):
  label_map[str(label[i])]=i

# Encode the categorical labels as numerical values using the label map
y_train_encoded = np.vectorize(label_map.get)(y_train)
y_val_encoded = np.vectorize(label_map.get)(y_val)

y_test_encoded = np.vectorize(label_map.get)(y_test)

# Convert the numerical labels to one-hot encoded format
num_classes = len(label)
y_train_onehot = keras.utils.to_categorical(y_train_encoded, num_classes=num_classes)
y_val_onehot = keras.utils.to_categorical(y_val_encoded, num_classes=num_classes)

y_test_onehot = keras.utils.to_categorical(y_test_encoded, num_classes=num_classes)


print(y_test_onehot.shape)
print(X_test.shape)

"""### DRL"""

pretrained_model = baseline_model
data_DRL=X_train
from tensorflow.keras.initializers import RandomNormal
from tensorflow.keras.layers import Conv2D

#  the environment
class ImageBlurDeblurEnv:

    def __init__(self, X=data_DRL, y=y_train_onehot):
        self.action_space = [0, 1, 2]
        # 0: blur, 1: deblur, 2: do nothing
        self.data_indices = np.arange(len(X))
        np.random.shuffle(self.data_indices)
        self.reset(0,X)

    def reset(self,i,X=data_DRL):
        if len(self.data_indices)==len(X):
          idx = self.data_indices[i]
        else:
          idx=i
        self.image = X[idx]
        initial_output = pretrained_model.predict(np.expand_dims(self.image, axis=0))
        self.threshold = np.max(initial_output, axis=1)[0]


        return self.image

    def step(self, action):
        if action == 0:
            self.image = cv2.GaussianBlur(self.image, (5, 5), 0)
        elif action == 1:
            self.image = cv2.addWeighted(self.image, 1.5, cv2.GaussianBlur(self.image, (5, 5), 0), -0.5, 0)

        predicted_output = pretrained_model.predict(np.expand_dims(self.image, axis=0))
        #self.predicted_prob_class = predicted_output[np.argmax(predicted_output, axis=1)]
        self.predicted_prob_class = np.max(predicted_output, axis=1)[0]
        self.predicted_class=np.argmax(predicted_output, axis=1)

        if self.threshold <= self.predicted_prob_class:
          reward = 1
          self.threshold = self.predicted_prob_class
        else:
          reward = -1
        done = True
        #save
        return self.image, reward, done


# the DQN model
def create_q_model(input_shape, num_actions):
    model = Sequential()
    model.add(Input(input_shape))
    model.add(Flatten())
    model.add(Dense(64, activation='relu', kernel_initializer=RandomNormal()))
    model.add(Dense(64, activation='relu', kernel_initializer=RandomNormal()))
    model.add(Dense(num_actions, kernel_initializer=RandomNormal()))
    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')
    return model


# replay buffer
class ReplayBuffer:
    def __init__(self, buffer_size):
        self.buffer = deque(maxlen=buffer_size)

    def add(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)

# Training parameters
EPISODES = 150
BATCH_SIZE = 20
GAMMA = 0.99
EPSILON_START = 1.0
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.995
BUFFER_SIZE = 100
TARGET_UPDATE_FREQ = 30

print("EPISODES:", EPISODES)
print("BATCH_SIZE:", BATCH_SIZE)
print("GAMMA:", GAMMA)
print("EPSILON_START:", EPSILON_START)
print("EPSILON_MIN:", EPSILON_MIN)
print("EPSILON_DECAY:", EPSILON_DECAY)
print("BUFFER_SIZE:", BUFFER_SIZE)
print("TARGET_UPDATE_FREQ:", TARGET_UPDATE_FREQ)

# Create environment, models, and buffer

env = ImageBlurDeblurEnv(data_DRL, y_train_onehot)
input_shape = (64, 64, 3)
num_actions = len(env.action_space)

q_model = create_q_model(input_shape, num_actions)
target_model = create_q_model(input_shape, num_actions)
target_model.set_weights(q_model.get_weights())
buffer = ReplayBuffer(BUFFER_SIZE)


epsilon = EPSILON_START



"""#### Validation set DQL"""

'''
learning_rates = [0.001]
discount_factors = [0.99]

validation_data = X_val

best_performance = -float('inf')
best_hyperparameters = {}

for lr in learning_rates:
    for gamma in discount_factors:
        # Create environment, models, and buffer
        env = ImageBlurDeblurEnv(data_DRL)
        input_shape = (64, 64, 3)
        num_actions = len(env.action_space)

        q_model = create_q_model(input_shape, num_actions)
        target_model = create_q_model(input_shape, num_actions)
        target_model.set_weights(q_model.get_weights())
        buffer = ReplayBuffer(BUFFER_SIZE)

        # Set hyperparameters
        q_model.compile(optimizer=Adam(learning_rate=lr), loss='mse')
        gamma = gamma

        epsilon = EPSILON_START

        for image_idx in range(len(data_DRL)):
            state = env.reset(image_idx, data_DRL)
            for episode in range(EPISODES):
                done = False
                while not done:
                    # Choose action
                    if np.random.rand() < epsilon:
                        action = np.random.choice(env.action_space)
                    else:
                        q_values = q_model.predict(np.expand_dims(state, axis=0))
                        action = np.argmax(q_values)
                    # Perform action, store experience, train the model, decay epsilon, update target model

                    # Perform action
                    next_state, reward, done = env.step(action)

                    # Store experience in buffer
                    buffer.add(state, action, reward, next_state, done)

                    state = next_state

                  # Train the model if the buffer has enough samples
                if len(buffer) >= BATCH_SIZE:
                    batch = buffer.sample(BATCH_SIZE)
                    states, actions, rewards, next_states, dones = zip(*batch)

                    states = np.array(states)
                    actions = np.array(actions)
                    rewards = np.array(rewards)
                    next_states = np.array(next_states)
                    dones = np.array(dones, dtype=bool)

                    next_q_values = target_model.predict(next_states)
                    target_q_values = q_model.predict(states)

                    target_q_values[np.arange(BATCH_SIZE), actions] = rewards + (1 - dones) * gamma * np.amax(next_q_values, axis=1)

                    q_model.train_on_batch(states, target_q_values)

                # Decay epsilon
                epsilon = max(epsilon * EPSILON_DECAY, EPSILON_MIN)

                  # Update target model
                if episode % TARGET_UPDATE_FREQ == 0:
                    target_model.set_weights(q_model.get_weights())

                if episode % 50 == 0:
                    print(f'Episode {episode} completed.')


        # Evaluate performance on the validation set
        total_reward = 0
        for image_idx in range(len(validation_data)):
            state = env.reset(image_idx, validation_data)
            done = False
            while not done:
                # Get the action with the highest Q-value using the trained DQL model
                q_values = q_model.predict(np.expand_dims(state, axis=0))
                action = np.argmax(q_values)

                # Perform the action in the environment
                next_state, reward, done = env.step(action)

                total_reward += reward

                state = next_state

        # Check if this hyperparameter combination performs better
        if total_reward > best_performance:
            best_performance = total_reward
            best_hyperparameters = {'learning_rate': lr, 'discount_factor': gamma}
'''

'''
# Train the final model with best hyperparameters
best_hyperparameters = {'learning_rate': 0.001, 'discount_factor': 0.99}
env = ImageBlurDeblurEnv(data_DRL)
q_model = create_q_model(input_shape, num_actions)
target_model = create_q_model(input_shape, num_actions)
target_model.set_weights(q_model.get_weights())
buffer = ReplayBuffer(BUFFER_SIZE)
print("Best Parameters:", best_hyperparameters)
q_model.compile(optimizer=Adam(learning_rate=best_hyperparameters['learning_rate']), loss='mse')
gamma = best_hyperparameters['discount_factor']

# Rest of the training loop for the final model
epsilon = EPSILON_START

for image_idx in range(len(data_DRL)):

    state = env.reset(image_idx, data_DRL)
    for episode in range(EPISODES):
        done = False
        while not done:
            # Choose action
            if np.random.rand() < epsilon:
                action = np.random.choice(env.action_space)
            else:
                q_values = q_model.predict(np.expand_dims(state, axis=0))
                action = np.argmax(q_values)
            # Perform action, store experience, train the model, decay epsilon, update target model
            # Perform action
            next_state, reward, done = env.step(action)

            # Store experience in buffer
            buffer.add(state, action, reward, next_state, done)

            state = next_state

            # Train the model if the buffer has enough samples
            if len(buffer) >= BATCH_SIZE:
                batch = buffer.sample(BATCH_SIZE)
                states, actions, rewards, next_states, dones = zip(*batch)

                states = np.array(states)
                actions = np.array(actions)
                rewards = np.array(rewards)
                next_states = np.array(next_states)
                dones = np.array(dones, dtype=bool)

                next_q_values = target_model.predict(next_states)
                target_q_values = q_model.predict(states)

                target_q_values[np.arange(BATCH_SIZE), actions] = rewards + (1 - dones) * gamma * np.amax(next_q_values, axis=1)

                q_model.train_on_batch(states, target_q_values)

            # Decay epsilon
            epsilon = max(epsilon * EPSILON_DECAY, EPSILON_MIN)

            # Update target model
            if episode % TARGET_UPDATE_FREQ == 0:
                target_model.set_weights(q_model.get_weights())


    if episode % 50 == 0:
        print('Episode completed:', episode)

'''
'''
# Assume BUFFER_SIZE, EPSILON_START, EPSILON_DECAY, EPSILON_MIN, EPISODES, TARGET_UPDATE_FREQ, and BATCH_SIZE are defined somewhere earlier in your code

best_hyperparameters = {'learning_rate': 0.001, 'discount_factor': 0.99}
env = ImageBlurDeblurEnv(data_DRL)
q_model = create_q_model(input_shape, num_actions)
target_model = create_q_model(input_shape, num_actions)
target_model.set_weights(q_model.get_weights())
buffer = ReplayBuffer(BUFFER_SIZE)
print("Best Parameters:", best_hyperparameters)
q_model.compile(optimizer=Adam(learning_rate=best_hyperparameters['learning_rate']), loss='mse')
gamma = best_hyperparameters['discount_factor']

epsilon = EPSILON_START

for image_idx in range(len(data_DRL)):
    print(image_idx)
    state = env.reset(image_idx, data_DRL)
    for episode in range(EPISODES):
        done = False
        while not done:
            if np.random.rand() < epsilon:
                action = np.random.choice(env.action_space)
            else:
                q_values = q_model.predict(np.expand_dims(state, axis=0))
                action = np.argmax(q_values)

            next_state, reward, done = env.step(action)

            buffer.add(state, action, reward, next_state, done)

            state = next_state

            if len(buffer) >= BATCH_SIZE:
                batch = buffer.sample(BATCH_SIZE)
                states, actions, rewards, next_states, dones = zip(*batch)

                states = np.array(states)
                actions = np.array(actions)
                rewards = np.array(rewards)
                next_states = np.array(next_states)
                dones = np.array(dones, dtype=bool)

                next_q_values = target_model.predict(next_states)
                target_q_values = q_model.predict(states)

                target_q_values[np.arange(BATCH_SIZE), actions] = rewards + (1 - dones) * gamma * np.amax(next_q_values, axis=1)

                q_model.train_on_batch(states, target_q_values)

            epsilon = max(epsilon * EPSILON_DECAY, EPSILON_MIN)

            if episode % TARGET_UPDATE_FREQ == 0:
                target_model.set_weights(q_model.get_weights())

        if episode % 50 == 0:
            print('Episode completed:', episode)
'''

####ALLL IMAGES IN EACH EPISODE ####


# Assume BUFFER_SIZE, EPSILON_START, EPSILON_DECAY, EPSILON_MIN, EPISODES, TARGET_UPDATE_FREQ, and BATCH_SIZE are defined somewhere earlier in your code

best_hyperparameters = {'learning_rate': 0.001, 'discount_factor': 0.99}
env = ImageBlurDeblurEnv(data_DRL)
q_model = create_q_model(input_shape, num_actions)
target_model = create_q_model(input_shape, num_actions)
target_model.set_weights(q_model.get_weights())
buffer = ReplayBuffer(BUFFER_SIZE)
print("Best Parameters:", best_hyperparameters)
q_model.compile(optimizer=Adam(learning_rate=best_hyperparameters['learning_rate']), loss='mse')
gamma = best_hyperparameters['discount_factor']

epsilon = EPSILON_START

for episode in range(EPISODES):
    for image_idx in range(len(data_DRL)):
        print('Image index:', image_idx)
        state = env.reset(image_idx, data_DRL)
        done = False
        while not done:
            if np.random.rand() < epsilon:
                action = np.random.choice(env.action_space)
            else:
                q_values = q_model.predict(np.expand_dims(state, axis=0))
                action = np.argmax(q_values)

            next_state, reward, done = env.step(action)

            buffer.add(state, action, reward, next_state, done)

            state = next_state

            if len(buffer) >= BATCH_SIZE:
                batch = buffer.sample(BATCH_SIZE)
                states, actions, rewards, next_states, dones = zip(*batch)

                states = np.array(states)
                actions = np.array(actions)
                rewards = np.array(rewards)
                next_states = np.array(next_states)
                dones = np.array(dones, dtype=bool)

                next_q_values = target_model.predict(next_states)
                target_q_values = q_model.predict(states)

                target_q_values[np.arange(BATCH_SIZE), actions] = rewards + (1 - dones) * gamma * np.amax(next_q_values, axis=1)

                q_model.train_on_batch(states, target_q_values)

            epsilon = max(epsilon * EPSILON_DECAY, EPSILON_MIN)

        if episode % TARGET_UPDATE_FREQ == 0:
            target_model.set_weights(q_model.get_weights())

    if episode % 50 == 0:
        print('Episode completed:', episode)

from tensorflow import keras
TryNumber='CX32'
filename = 'qmodel_{}.h5'.format(TryNumber)
# Specify the absolute file path for saving the model
#save_path = os.path.join('/content/drive/MyDrive/Thesis Folder/Dataset/Subset105/', filename)
q_model.save(filename)

"""#### Test DQL"""
"""#### val

"""

import numpy as np


def test_dql_agent(dql_agent, data_X, y, env):
  history_images = pd.DataFrame(columns=['Image_Number', '#Steps', 'Action', 'New image', 'reward', 'Previous img',"pred_class"])
  episode_rewards = []
  episode_lengths = []
  cumulative_rewards = []

  for img in range(len(data_X)):
    state = env.reset(img, data_X)
    #for episode in range(EPISODES):
    done = False
    total_reward = 0
    step_count = 0

    while not done:
        # Get the action with the highest Q-value using the trained DQL model
        q_values = dql_agent.predict(np.expand_dims(state, axis=0))
        action = np.argmax(q_values)

        # Perform the action in the environment
        next_state, reward, done = env.step(action)
        pred_class = env.predicted_class

        # Add a new row using append
        new_data = {'Image_Number': img, '#Steps': step_count+1 , 'Action': action,"Real class":np.argmax(y[img]), 'New image': next_state, 'reward': reward,'Previous img':state,"pred_class": pred_class}
        history_images = history_images.append(new_data, ignore_index=True)

        total_reward += reward
        step_count += 1
        state = next_state

    episode_rewards.append(total_reward)
    episode_lengths.append(step_count)
        # Calculate cumulative reward
    if cumulative_rewards:
        cumulative_rewards.append(cumulative_rewards[-1] + total_reward)
    else:
        cumulative_rewards.append(total_reward)

  return cumulative_rewards, episode_rewards, episode_lengths, history_images

cumulative_rewards_val, episode_rewards_val, episode_lengths_val, history_images_val = test_dql_agent(q_model, X_val, y_val_onehot, env)

# Calculate and print performance metrics

dic_results= {'cumulative rewards val':cumulative_rewards_val, 'episode_rewards val': episode_rewards_val, 'episode_lengths val': episode_lengths_val}
filename = 'rewards_leng_dql_val{}.pkl'.format(TryNumber)
with open(filename, "wb") as file:
    pickle.dump(dic_results, file)

mean_reward = np.mean(episode_rewards_val)
mean_length = np.mean(episode_lengths_val)
print(f"Mean episode reward val: {mean_reward:.2f}")
print(f"Mean episode length val: {mean_length:.2f}")

import matplotlib.pyplot as plt

# Assuming episode_rewards is a list containing the cumulative reward for each episode
plt.figure(figsize=(10, 5))
plt.plot(cumulative_rewards_val)
plt.title('Cumulative Reward per Episode')
plt.xlabel('Episode')
plt.ylabel('Cumulative Reward')

filename = 'cumulative_rewards_val{}.png'.format(TryNumber)
plt.savefig(filename)

plt.show()

filename = 'historyimages_val{}.pkl'.format(TryNumber)
with open(filename, "wb") as file:
    pickle.dump(history_images_val, file)

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report

new_img = np.stack(history_images_val["New image"].values)
y_pred = baseline_model.predict(new_img)
y_pred_classes = np.argmax(y_pred, axis=1)

y_pred_classes=keras.utils.to_categorical(y_pred_classes, num_classes=num_classes)
filename = 'predictionDQL_{}.pkl'.format(TryNumber)
with open(filename, "wb") as file:
    pickle.dump(y_pred_classes, file)

y_real=keras.utils.to_categorical(history_images_val["Real class"], num_classes=num_classes)
accuracy = accuracy_score(y_real, y_pred_classes )
print(classification_report(y_real, y_pred_classes ))
print("Accuracy with DQL Validation: {:.2f}%".format(accuracy * 100))

"""#### test"""

import numpy as np


def test_dql_agent(dql_agent, data_X, y, env):
  history_images = pd.DataFrame(columns=['Image_Number', '#Steps', 'Action', 'New image', 'reward', 'Previous img',"pred_class"])
  episode_rewards = []
  episode_lengths = []
  cumulative_rewards = []

  for img in range(len(data_X)):
    state = env.reset(img, data_X)
    #for episode in range(EPISODES):
    done = False
    total_reward = 0
    step_count = 0

    while not done:
        # Get the action with the highest Q-value using the trained DQL model
        q_values = dql_agent.predict(np.expand_dims(state, axis=0))
        action = np.argmax(q_values)

        # Perform the action in the environment
        next_state, reward, done = env.step(action)
        pred_class = env.predicted_class

        # Add a new row using append
        new_data = {'Image_Number': img, '#Steps': step_count+1 , 'Action': action,"Real class":np.argmax(y[img]), 'New image': next_state, 'reward': reward,'Previous img':state,"pred_class": pred_class}
        history_images = history_images.append(new_data, ignore_index=True)

        total_reward += reward
        step_count += 1
        state = next_state

    episode_rewards.append(total_reward)
    episode_lengths.append(step_count)
        # Calculate cumulative reward
    if cumulative_rewards:
        cumulative_rewards.append(cumulative_rewards[-1] + total_reward)
    else:
        cumulative_rewards.append(total_reward)

  return cumulative_rewards, episode_rewards, episode_lengths, history_images

start_time = time.time()
cumulative_rewards, episode_rewards, episode_lengths, history_images = test_dql_agent(q_model, X_test, y_test_onehot, env)
end_time = time.time()
duration_time_dql = end_time - start_time
# Calculate and print performance metrics

dic_results= {'cumulative rewards':cumulative_rewards, 'episode_rewards': episode_rewards, 'episode_lengths': episode_lengths}
filename = 'rewards_leng_dql_{}.pkl'.format(TryNumber)
with open(filename, "wb") as file:
    pickle.dump(dic_results, file)

mean_reward = np.mean(episode_rewards)
mean_length = np.mean(episode_lengths)
print(f"Mean episode reward: {mean_reward:.2f}")
print(f"Mean episode length: {mean_length:.2f}")

"""## Evaluation Accuracy

"""

print("EVALUATION DQL")

print(f"Mean cumulative reward: {mean_reward:.2f}")
print(f"Mean episode length: {mean_length:.2f}")

import matplotlib.pyplot as plt

# Assuming episode_rewards is a list containing the cumulative reward for each episode
plt.figure(figsize=(10, 5))
plt.plot(cumulative_rewards)
plt.title('Cumulative Reward per Episode')
plt.xlabel('Episode')
plt.ylabel('Cumulative Reward')

filename = 'cumulative_rewards{}.png'.format(TryNumber)
plt.savefig(filename)

plt.show()

import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))
plt.plot(episode_lengths)
plt.title('Episode Length over Time')
plt.xlabel('Episode')
plt.ylabel('Episode Length')
filename = 'episode_lengths{}.png'.format(TryNumber)
plt.savefig(filename)
plt.show()

plt.figure(figsize=(10, 5))
plt.plot(episode_rewards)
plt.title('Episode Rewards')
plt.xlabel('Episode')
plt.ylabel('Reward')
filename = 'Episode_Rewards_{}.png'.format(TryNumber)
plt.savefig(filename)

plt.show()

filename = 'historyimages_{}.pkl'.format(TryNumber)
with open(filename, "wb") as file:
    pickle.dump(history_images, file)

history_images.head()

history_images.tail()

history_images.iloc[15,:]

print(len(history_images))
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
start_time=time.time()
new_img = np.stack(history_images["New image"].values)
y_pred = baseline_model.predict(new_img)
y_pred_classes = np.argmax(y_pred, axis=1)
end_time=time.time()
duration_time_dql=duration_time_dql+ (end_time - start_time)
print("Time taken for Predictions with DQL-CNN :", duration_time_dql, "seconds")


y_pred_classes=keras.utils.to_categorical(y_pred_classes, num_classes=num_classes)
filename = 'predictionDQL_{}.pkl'.format(TryNumber)
with open(filename, "wb") as file:
    pickle.dump(y_pred_classes, file)

y_real=keras.utils.to_categorical(history_images["Real class"], num_classes=num_classes)
accuracy = accuracy_score(y_real, y_pred_classes )
print(classification_report(y_real, y_pred_classes ))
print("Accuracy with DQL: {:.2f}%".format(accuracy * 100))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
y_true_classes = y_real.argmax(axis=1)
y_pred_classes = y_pred_classes.argmax(axis=1)
cm = confusion_matrix(y_true_classes, y_pred_classes)


plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')

filename = 'confusion_matrix_dqlcnn_{}.png'.format(TryNumber)
plt.savefig(filename)
plt.show()

# Record the start time
start_time = time.time()

# Make predictions on the test set CNN
y_pred = baseline_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)

# Record the end time
end_time = time.time()

# Calculate the duration in seconds
duration = end_time - start_time

# Print the duration
print("Time taken for Predictions with only CNN :", duration, "seconds")

y_pred_classes=keras.utils.to_categorical(y_pred_classes, num_classes=num_classes)

filename = 'predictionCNN_{}.pkl'.format(TryNumber)
with open(filename, "wb") as file:
    pickle.dump(y_pred_classes, file)


# Calculate
accuracy = accuracy_score(y_test_onehot, y_pred_classes)
print(classification_report(y_test_onehot, y_pred_classes))



print("Accuracy Only CNN: {:.2f}%".format(accuracy * 100))

from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
y_test_classes = y_test_onehot.argmax(axis=1)
y_pred_classes = y_pred_classes.argmax(axis=1)

cm = confusion_matrix(y_test_classes, y_pred_classes)

#cm = confusion_matrix((y_test_onehot, y_pred_classes))
plt.figure(figsize=(10, 7))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')

filename = 'confusion_matrix_CNN_{}.png'.format(TryNumber)
plt.savefig(filename)
plt.show()
